{
  "hash": "fe2282eea8ba2ef20a5a3c72fc0b6670",
  "result": {
    "markdown": "---\ntitle: \"Historical low temperatures in Fort Collins, Colorado\"\ndate: 2023-02-10\nauthor: \n  - name: Daniel Burcham\n    orcid: 0000-0002-1793-3945\ndescription: \"Use the `extRemes` R package to fit extreme value distributions to daily low temperatures\"\nimage: Effective-100-year-return-levels.png\ncategories:\n  - r\n  - tidyverse\n  - ggplot\n  - extreme values\n---\n\n::: {.cell}\n\n:::\n\n\n## The dead of winter\n\nRecently, the Colorado State University weather station recorded a daily\nlow temperature of -15.4° F in the early morning of 22 December 2022. At\nthe time, wind chills varying between -50° and -35° F threatened lives\nacross most of the Eastern plains in Colorado. The frigid temperatures\naccompanied a string of unpleasant winter weather for much of the US\nwith many places experiencing bitter cold, gusty winds, and heavy snow;\nand the unfortunate timing of the weather only compounded the\nfrustrations of many people traveling over the holidays.\n\nCold weather snaps are common in Colorado, and the plants and people\nliving here must equally tolerate occasional severe freezing.\nFortunately, many trees easily endure freezing temperatures during the\nwinter months - the vegetative buds on most deciduous broadleaf trees\nwithstand temperatures between -13°F and -37° F (-25° and -35° C).\nOccasionally, though, some cultivated trees are injured or killed by low\ntemperatures, especially if temperatures drop abruptly after relatively\nwarm weather (e.g., *false springs*) or the species was introduced from\na milder climate. The sudden freezes can be especially harmful if trees\nare not acclimated to cold temperatures; the events can damage buds,\nleaves, and flowers and, in some cases, disrupt water conduction or\nnatural growth patterns. In recent years, for example, several freeze\nevents killed large numbers of Siberian elms (*Ulmus pumila*) along the\nFront Range, and I have heard similar stories about widespread tree\ndamage caused by unusual fall freezes in earlier decades.\n\n## Freezing tolerance in trees\n\nTrees use a number of strategies to tolerate freezing temperatures\nduring winter, including withdrawing water into non-living tissues where\nice formation avoids damage to living cells, lowering the freezing point\nof the water retained in cells, and forming physical barriers\nrestricting the propagation of ice crystals. As temperatures fall below\nfreezing, extracellcular ice draws out some of the water retained in\nliving cells along an osmotic gradient between the two phases of water,\nand trees further modify their living cells to tolerate the dehydration\nduring freezing. In addition to leaf shedding, the modifications are all\npart of a tree's yearly preparations for the winter season.\nUnfortunately, smooth transitions are not part of Colorado's weather\npatterns. Highly variable and adverse weather conditions prevail over\nthe region's semi-arid landscapes, and trees unaccustomed to such\nvariability may be at greater risk of freeze damage.\n\nThe seasonal pattern of cold hardiness is U-shaped with trees\nincreasingly hardy to lower freezing temperatures during fall,\nconsistently hardy to minimum temperatures during the winter, and\ndecreasingly hardy during the spring. Crucially, the timing of seasonal\ntransitions ensures that a tree's physiological activity matches\nchanging environmental conditions. There is broad agreement that day\nlength and temperature mainly govern the acquisition and loss of cold\nhardiness, respectively, in the fall and spring each year, but\nindividual tree species also display a unique sensitivity to the two\nseasonal environmental cues for spring emergence. Some species, like\ngreen ash (*Fraxinus pennsylvanica*) and Siberian elm, are more\nsensitive to temperature during bud burst in the spring, and others,\nlike white ash (*Fraxinus americana*) and littleleaf linden (*Tilia\ncordata*), are more sensitive to day length during the same transition.\nThe heavy reliance on fickle temperatures for seasonal transitions may\nexplain the winter damage more commonly observed on the former two\nspecies, but the seasonal cues for many species are also moderated by a\nchilling (accumulated low temperature) requirement that, if unfulfilled,\nprevents premature bud burst.\n\nFor a given species, the precise timing of spring budburst is determined\nby the maximum cold hardiness, low winter temperatures (chilling), warm\nspring temperatures (forcing), and increasing spring day length\n(photoperiod). In many places, scientists have observed a steady\nadvancement of biological spring towards earlier times of the year amid\nthe warming climate, and the risk of freeze damage during false springs\nmay be especially significant for trees, depending on the rate of cold\nhardiness loss during spring. In Colorado, wild temperature fluctuations\nare already commonplace in the spring and fall, and many people have\nlearned to simply avoid trees tending to leaf out early or retain leaves\nlate in a season.\n\n## Statistics of extremes\n\nRecently, I received a summary of extreme low temperatures in Fort\nCollins in the 20^th^ Century, and the report clearly showed that our\npredecessors on the Front Range endured *much* colder days. In fact, it\nis difficult to imagine the landscape and conditions encountered by\nearly settlers before all the roads, houses, and McDonald's. The small\npatches of preserved steppe offer mere glimpses of pre-settlement\nlandscapes, but the near complete absence of trees from the native\nshortgrass prairie reflects the region's poor suitability for large\nwoody plants. Without deliberate care from people, our community forests\nwould not thrive. Curious about historical temperature trends, I decide\nto update the summary with more recent observations and explore the use\nof extreme value analysis to characterize severe freezing events in Fort\nCollins using the `extRemes` package in `R`. Instead of evaluating more\ncommonplace conditions, extreme value distributions depict the\noccurrence of maximum (or minimum) values in a data set. Usefully, they\ncan statistically characterize extreme climate processes without a\nmechanistic treatment of the underlying physical phenomena. They have\nbeen used to describe the probability of very rare or extreme events,\nsuch as severe \"100-year\" floods, and they have yielded important design\ncriteria for engineers anticipating the limits of environmental\nconditions affecting buildings or infrastructure. For many, the models\ncan also be confusing when the frequency or magnitude of real world\nevents differs significantly from predictions based on historical data.\n\n![Marker in Creekside Park for the 1997 flood in Fort Collins](DSCF6217.jpg){#fig-marker width=50%} \n\nFor example, I often ride past a marker in\nCreekside Park (@fig-marker) showing the magnitude of a severe flood on July 28, 1997\nin Fort Collins. After a torrent of rain fell in a short period of time,\nSpring Creek overflowed and washed out large areas of the community. Ultimately, the storm caused millions in property\ndamage, dozens of injuries, and five fatalities. The marker depicts the\nunprecedented magnitude of the flood by comparing it to the much smaller\n5-, 10-, and 100-year flood levels.\n\nDespite its longstanding use in the physical sciences, the techniques\ncan also be used to understand biological phenomena. I first encountered\nextreme value analysis in a\n[study](https://doi.org/10.4319/lo.1990.35.1.0001) examining the\nstochastic forces of crashing waves on limpets in rocky shores, and the\nauthors sought to quantify the probability of an extreme wave-induced\nforce potentially dislodging the aquatic snails. The cumulative density\nfunction (CDF) for the Generalized Extreme Value (GEV) distribution can\nbe defined as:\n$$F(x;\\mu,\\sigma,\\gamma)=e^{-[1-\\gamma(x-\\mu)/\\sigma]^{1/\\gamma}}$$ {#eq-GEVD-CDF}\nwhere $\\gamma\\ne0$ and $[1-\\gamma(x-\\mu)/\\sigma]>0$. The three\nparameters $\\mu$, $\\sigma$, and $\\gamma$ depict the location, scale, and\nshape of the distribution. The location parameter, $\\mu$, represents the\nmost common, i.e., modal, extreme value. The scale parameter, $\\sigma$,\nportrays the rate of change in $x$ with the natural logarithm of time,\nand the ration $\\sigma/\\gamma$ represents the maximum extreme value. The\nreturn level, $x$, associated with a return period, $T$, corresponds to\nthe $1-p$ quantile of the distribution, where $p=1/T$. For example, a\n100-year return level would correspond to the $1-{1\\over100}=0.99$\nquantile. Return levels can be obtained using the quantile function:\n$$F^{-1}(1-p;\\mu,\\sigma,\\gamma)=\\mu+(\\sigma/\\gamma)\\{{1\\over[-ln(1-p)]^\\gamma}-1\\}$$ {#eq-quantile-function}\nwhere $\\gamma\\ne0$. The interpretation of a return level often causes\nconfusion, but it is simply the value expected to be exceeded, on\naverage, once every $T$ years.\n\nThe `extRemes` package was developed by Eric Gilleland, a CSU graduate\nnow working at the National Center for Atmospheric Research in Boulder,\nand it has facilitated greater interest and use of extreme value\nstatistics by many more people, including me! To update the data, I\nconsolidated the 20^th^ Century weather data from Fort Collins contained\nin `extRemes` with more recent observations from the same weather\nstation. Today, the weather station is situated on the main campus of\nCSU near the Lory Student Center, but the station was initially operated\nnear the former \"Old Main\" building with observations starting on 1\nJanuary 1889. Maintained by the Department of Atmospheric Science, the\nstation offers one of the oldest weather records in the state.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyverse)\nlibrary(ggtext)\nlibrary(extRemes)\nlibrary(gridExtra)\nlibrary(kableExtra)\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(modelsummary)\nlibrary(showtext)\n\n# Load FCwx data from extRemes package\ndata(FCwx)\n\n# Query updated FCwx observations and combine\napi <- GET(\"https://coagmet.colostate.edu/data/daily/fcl01.json?from=2000-01-01&to=2022-12-31&fields=tMax,tMin,precip\")\nFCwx2k <- do.call(cbind.data.frame,fromJSON(rawToChar(api$content))) |> \n  mutate(dt = as.Date(time,\"%Y-%m-%d\"), Dy = yday(dt), Mn = month(dt), Year = year(dt)) |>\n  select(dt,Dy,Mn,Year,tMin) |> rename(MnT = tMin)\n\nFCwx <- FCwx |> mutate(dt = as.Date(paste(Dy,\"-\",Mn,\"-\",Year),\"%d - %m - %Y\")) |>\n  select(dt,Dy,Mn,Year,MnT)\n\nFCwx <- bind_rows(FCwx,FCwx2k) |> mutate(doy = yday(dt))\nFCwx <- FCwx[FCwx$MnT != -999,]\n\n#windowsFonts(Inter = windowsFont(\"Inter\"))\nfont_add_google(\"Inter\",\"inter\")\nshowtext_auto()\n\ntheme_nice <- function() {\n  theme_minimal(base_family = \"inter\") + \n    theme(panel.grid.minor = element_blank(),\n          panel.spacing.x = unit(25, \"points\"),\n          plot.title = element_text(family= \"inter\", face = \"bold\"),\n          axis.title = element_text(family = \"inter\"),\n          strip.text = element_text(family = \"inter\", face = \"bold\",\n                                    size = rel(1), hjust = 0),\n          strip.background = element_rect(fill = \"grey80\", color = NA))\n}\n\nupdate_geom_defaults(\"label\", list(family=\"inter\"))\nupdate_geom_defaults(ggtext::GeomRichText, list(family=\"inter\"))\n```\n:::\n\n\nUpon inspection, the data revealed some obvious patterns. Between\n1900 and 2022, there\nwere 19,632 days with\nlow temperatures below freezing in Fort Collins (approximately 44% of\nall observed days). Recorded on\n01 February 1951, the coldest\ndaily low was -41° F! Wow, that's really cold. Near the\nlower limit of the observed range, there were\n73 days with low temperatures below\n-20° F, but such extremely cold days were not observed uniformly over\nthe past 122 years:\n79.5%\nof the events occurred before 1950.\n\n\n::: {.cell layout-align=\"center\" fig.showtext='true'}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(data=FCwx,aes(x=dt,y=MnT)) + \n  geom_point(alpha=0.5,position=\"jitter\") + \n  geom_smooth() + \n  xlab(\"Date\") + ylab(\"Daily Minimum Temperature\") + \n  theme_nice()\n```\n\n::: {.cell-output-display}\n![Daily minimum temperatures in Fort Collins, CO between 1900 and 2022](index_files/figure-html/fig-global-plot-1.png){#fig-global-plot fig-align='center' width=90%}\n:::\n:::\n\n\nIn the entire data record (@fig-global-plot), there is an obvious upward\ntrend in daily minimum temperatures over time, and you can also clearly\nsee more variability around the lower limit of observations. As a\nconvenient starting point, I simply fit a GEV distribution to all\nobservations. The location parameter for the preliminary fit, for\nexample, indicated that the most common daily minimum temperature was\n41.6° F (@tbl-fit0). For such cases, one important modeling assumption\nrequires the use of homogeneous data obtained from a process *not*\nundergoing any systematic change. In many cases, however, extreme value\nprocesses exhibit slowly-varying or cyclical behavior, and the\nprobability of an extreme event, often, varies according to diurnal,\nseasonal, or annual conditions. Apart from significant seasonal\nvariation, the long-term trend towards warmer daily low temperatures in\nFort Collins likely violates this assumption.\n\n\n::: {#tbl-fit0 .cell layout-align=\"center\" tbl-cap='Parameter estimates for stationary Generalized Extreme Value distribution fit to negative daily minimum temperatures in Fort Collins, CO between 1900 and 2022'}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Fit stationary model\nfit0 <- fevd(-MnT ~ 1, FCwx, type = \"GEV\", span = 123, units = \"deg F\", time.units = \"days\", period.basis = \"year\")\n\n# Stationary model summary table\nfit0.summary <- summary(fit0, silent=TRUE)\nparams.ci.fit0 <- data.frame(matrix(ci(fit0, type = \"parameter\"),ncol=3))\ncolnames(params.ci.fit0) <- c(\"ll\",\"est\",\"ul\")\nfit0.model.summary <- params.ci.fit0 |> \n  mutate(estimate = paste(round(params.ci.fit0$est,digits=2),\" (\",round(params.ci.fit0$ll,digits=2),\", \",round(params.ci.fit0$ul,digits=2),\")\", sep = \"\")) |>\n  select(estimate)\nfit0.model.summary <- data.frame(params = c(\"Location, &#956;\",\"Scale, &#963;\",\"Shape, &#947;\"), fit0.model.summary)\n\nfootnote(kbl(fit0.model.summary, format=\"html\", booktabs=TRUE, col.names=c(\"Parameters\", \"Estimate (95% CI)\"), row.names=FALSE, digits=2, align=\"lc\", escape=FALSE) |> \n  column_spec(1,width=\"10em\") |>\n  column_spec(2,width=\"12em\") |> \n  kable_styling(full_width = FALSE, position=\"left\"), paste(\"Negative log-likelihood (NLLH): \",round(fit0$results$value,2),\"; Bayesian Information Criterion (BIC): \",round(fit0.summary$BIC,2), sep=\"\"), footnote_as_chunk = TRUE)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; border-bottom: 0;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Parameters </th>\n   <th style=\"text-align:center;\"> Estimate (95% CI) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;width: 10em; \"> Location, μ </td>\n   <td style=\"text-align:center;width: 12em; \"> -41.64 (-41.8, -41.48) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 10em; \"> Scale, σ </td>\n   <td style=\"text-align:center;width: 12em; \"> 15.26 (15.15, 15.38) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 10em; \"> Shape, γ </td>\n   <td style=\"text-align:center;width: 12em; \"> -0.14 (-0.14, -0.13) </td>\n  </tr>\n</tbody>\n<tfoot><tr><td style=\"padding: 0; \" colspan=\"100%\">\n<span style=\"font-style: italic;\">Note: </span> <sup></sup> Negative log-likelihood (NLLH): 189790.29; Bayesian Information Criterion (BIC): 379612.71</td></tr></tfoot>\n</table>\n\n`````\n:::\n:::\n\n\nFortunately, it is possible to account for non-stationary extremes by\ndirectly modeling variation in the distribution parameters. To explore\nvariation in the distribution parameters over time, I fit multiple\nstationary GEV distributions to short, overlapping five-year segments of\nthe data between 1902 and 2018.\n\n\n::: {.cell layout-align=\"center\" fig.showtext='true'}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Fit GEVD using running five-year windows between 1902 and 2018 and store result\n\nmnt.yrs <- list()\n\nfor (i in 1902:2018){\n  mnt.yrs[[i-1901]] <- fevd(-MnT ~ 1, FCwx |> filter(format(dt,\"%Y\") == seq(i-2,i+2)), type = \"GEV\", span = 5, units = \"deg F\", time.units = \"days\", period.basis = \"year\")\n}\n\nlocs.yrs <- data.frame(years = seq(1902,2018),locations = matrix(unlist(lapply(mnt.yrs,'[[',c(20,1,1)))))\nscls.yrs <- data.frame(years = seq(1902,2018),scales = matrix(unlist(lapply(mnt.yrs,'[[',c(20,1,2)))))\nshps.yrs <- data.frame(years = seq(1902,2018),shapes = matrix(unlist(lapply(mnt.yrs,'[[',c(20,1,3)))))\n\np1 <- ggplot(data=locs.yrs, aes(x=years, y=locations)) + \n  labs(x = \"Year\", y = \"Location, \\u03bc\", escape = FALSE) +\n  geom_hline(yintercept = -41.64) + \n  geom_hline(yintercept = -41.8, linetype = \"dashed\") + \n  geom_hline(yintercept = -41.48, linetype = \"dashed\") + \n  geom_point() + theme_nice() + \n  scale_x_continuous(breaks = c(1900, 1950, 2000))\np2 <- ggplot(data=scls.yrs, aes(x=years, y=scales)) + \n  labs(x = \"Year\", y = \"Scale, \\u03c3\") +\n  geom_hline(yintercept = 15.26) + \n  geom_hline(yintercept = 15.15, linetype = \"dashed\") + \n  geom_hline(yintercept = 15.38, linetype = \"dashed\") + \n  geom_point() + theme_nice() + \n  scale_x_continuous(breaks = c(1900, 1950, 2000))\np3 <- ggplot(data=shps.yrs, aes(x=years, y=shapes)) + \n  labs(x = \"Year\", y = \"Shape, \\u03b3\") +\n  geom_hline(yintercept = -0.136) + \n  geom_hline(yintercept = -0.143, linetype = \"dashed\") + \n  geom_hline(yintercept = -0.129, linetype = \"dashed\") + \n  geom_point() + theme_nice() + \n  scale_x_continuous(breaks = c(1900, 1950, 2000))\ngrid.arrange(p1,p2,p3,nrow=1)\n```\n\n::: {.cell-output-display}\n![Generalized Extreme Value distribution parameters fit to running five-year windows of daily minimum temperature in Fort Collins, CO between 1902 and 2018](index_files/figure-html/fig-moving-fit-1.png){#fig-moving-fit fig-align='center' width=90%}\n:::\n:::\n\n\nThe estimates show obvious variation in the location parameter over the\nexamined years with the modal (negative) daily low slowly decreasing\n(increasing) over time. This is consistent with the trend towards warmer\ndaily minimum temperatures observed in @fig-global-plot. Compared to the\nlocation parameter, the other two parameters do not similarly vary over\ntime. However, it's also completely obvious to expect seasonal variation\nin daily minimum temperatures, and a simple harmonic function can be\nused to model cyclical variation in seasonal lows. For the\nnon-stationary case, I fit two candidate models: one modelling annual\nand seasonal variation in the location parameter and a second modelling\nadditional seasonal variation in the scale parameter. In both models,\nthe location parameter was modeled using:\n$$\\mu=\\mu_0+\\mu_1cos(2\\pi*doy/365.25)+\\mu_2sin(2\\pi*doy/365.25)+\\mu_3*year$$ {#eq-fit-1}\nwhere $doy$ is the day of the year represented as an integer and $year$\nis the calendar year.\n\n\n::: {#tbl-fit1 .cell layout-align=\"center\" tbl-cap='Parameter estimates for non-stationary Generalized Extreme Value distribution fit to negative daily minimum temperatures in Fort Collins, CO between 1900 and 2022'}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Non-stationary model 1\nfit1 <- fevd(-MnT ~ 1, FCwx,location.fun = ~ cos(2*pi*doy/365.25) + sin(2*pi*doy/365.25) + Year, type = \"GEV\", span = 123, units = \"deg F\", time.units = \"days\", period.basis = \"year\")\n\n# Non-stationary model 1 summary table\nfit1.summary <- summary(fit1, silent=TRUE)\nparams.ci.fit1 <- data.frame(matrix(ci(fit1, type = \"parameter\"),ncol=3))\ncolnames(params.ci.fit1) <- c(\"ll\",\"est\",\"ul\")\nfit1.model.summary <- params.ci.fit1 |> \n  mutate(estimate = paste(round(params.ci.fit1$est,digits=2),\" (\",round(params.ci.fit1$ll,digits=2),\", \",round(params.ci.fit1$ul,digits=2),\")\", sep = \"\")) |>\n  select(estimate)\nfit1.model.summary <- data.frame(params = c(\"&#956;0\", \"&#956;1\", \"&#956;2\", \"&#956;3\", \"Scale, &#963;\",\"Shape, &#947;\"), fit1.model.summary)\n\nfootnote(kbl(fit1.model.summary, format=\"html\", booktabs=TRUE, col.names=c(\"Parameters\", \"Estimate (95% CI)\"), row.names=FALSE, digits=2, align=\"lc\", escape=FALSE) |> \n  column_spec(1,width=\"10em\") |>\n  column_spec(2,width=\"12em\") |>\n  pack_rows(\"Location, &#956;\", 1, 4, escape = FALSE) |> \n  kable_styling(full_width = FALSE, position = \"left\"), paste(\"Negative log-likelihood (NLLH): \",round(fit1$results$value,2),\"; Bayesian Information Criterion (BIC): \",round(fit1.summary$BIC,2),\"; See Equation 3 for the function used to model the location  parameter.\", sep = \"\"), footnote_as_chunk = TRUE)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; border-bottom: 0;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Parameters </th>\n   <th style=\"text-align:center;\"> Estimate (95% CI) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr grouplength=\"4\"><td colspan=\"2\" style=\"border-bottom: 1px solid;\"><strong>Location, μ</strong></td></tr>\n<tr>\n   <td style=\"text-align:left;width: 10em; padding-left: 2em;\" indentlevel=\"1\"> μ0 </td>\n   <td style=\"text-align:center;width: 12em; \"> 50.11 (44.87, 55.35) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 10em; padding-left: 2em;\" indentlevel=\"1\"> μ1 </td>\n   <td style=\"text-align:center;width: 12em; \"> 18.38 (18.28, 18.49) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 10em; padding-left: 2em;\" indentlevel=\"1\"> μ2 </td>\n   <td style=\"text-align:center;width: 12em; \"> 5.37 (5.28, 5.47) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 10em; padding-left: 2em;\" indentlevel=\"1\"> μ3 </td>\n   <td style=\"text-align:center;width: 12em; \"> -0.04 (-0.05, -0.04) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 10em; \"> Scale, σ </td>\n   <td style=\"text-align:center;width: 12em; \"> 7.17 (7.11, 7.23) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 10em; \"> Shape, γ </td>\n   <td style=\"text-align:center;width: 12em; \"> -0.1 (-0.1, -0.09) </td>\n  </tr>\n</tbody>\n<tfoot><tr><td style=\"padding: 0; \" colspan=\"100%\">\n<span style=\"font-style: italic;\">Note: </span> <sup></sup> Negative log-likelihood (NLLH): 155733.78; Bayesian Information Criterion (BIC): 311531.82; See Equation 3 for the function used to model the location  parameter.</td></tr></tfoot>\n</table>\n\n`````\n:::\n:::\n\n\nCompared to the stationary model, the BIC is about 18% lower for the\nnon-stationary mode, indicating a much better fit when the annual and\nseasonal variation in the location parameter was modeled. In the second\nmodel, the scale parameter was additionally modeled using:\n$$\\sigma=\\sigma_0+\\sigma_1cos(2\\pi*doy/365.25)+\\sigma_2sin(2\\pi*doy/365.25)$$ {#eq-fit-2}\n\n\n::: {#tbl-fit2 .cell layout-align=\"center\" tbl-cap='Parameter estimates for non-stationary Generalized Extreme Value distribution fit to negative daily minimum temperatures in Fort Collins, CO between 1900 and 2022'}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Non-stationary model 2\nfit2 <- fevd(-MnT ~ 1, FCwx,location.fun = ~ cos(2*pi*doy/365.25) + sin(2*pi*doy/365.25) + Year, scale.fun = ~ cos(2*pi*doy/365.25) + sin(2*pi*doy/365.25), use.phi = TRUE, type = \"GEV\", span = 123, units = \"deg F\", time.units = \"days\", period.basis = \"year\")\n\n# Non-stationary model 2 summary table\nfit2.summary <- summary(fit2, silent=TRUE)\nparams.ci.fit2 <- data.frame(matrix(ci(fit2, type = \"parameter\"),ncol=3))\ncolnames(params.ci.fit2) <- c(\"ll\",\"est\",\"ul\")\nfit2.model.summary <- params.ci.fit2 |> \n  mutate(estimate = paste(round(params.ci.fit2$est,digits=2),\" (\",round(params.ci.fit2$ll,digits=2),\", \",round(params.ci.fit2$ul,digits=2),\")\", sep = \"\")) |>\n  select(estimate)\nfit2.model.summary <- data.frame(params = c(\"&#956;0\", \"&#956;1\", \"&#956;2\", \"&#956;3\", \"&#963;0\", \"&#963;1\", \"&#963;2\", \"Shape, &#947;\"), fit2.model.summary)\n\nfootnote(kbl(fit2.model.summary, format=\"html\", booktabs=TRUE, col.names=c(\"Parameters\", \"Estimate (95% CI)\"), row.names=FALSE, digits=2, align=\"lc\", escape=FALSE) |> \n  column_spec(1,width=\"10em\") |>\n  column_spec(2,width=\"12em\") |>\n  pack_rows(\"Location, &#956;\", 1, 4, escape = FALSE) |> \n  pack_rows(\"Scale, &#963;\", 5, 7, escape = FALSE) |> \n  kable_styling(full_width = FALSE, position = \"left\"), paste(\"Negative log-likelihood (NLLH): \",round(fit2$results$value,2),\"; Bayesian Information Criterion (BIC): \",round(fit2.summary$BIC,2),\"; See Equation 3 and Equation 4 for the functions used to model the location and scale parameter, respectively.\", sep = \"\"), footnote_as_chunk = TRUE)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; border-bottom: 0;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Parameters </th>\n   <th style=\"text-align:center;\"> Estimate (95% CI) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr grouplength=\"4\"><td colspan=\"2\" style=\"border-bottom: 1px solid;\"><strong>Location, μ</strong></td></tr>\n<tr>\n   <td style=\"text-align:left;width: 10em; padding-left: 2em;\" indentlevel=\"1\"> μ0 </td>\n   <td style=\"text-align:center;width: 12em; \"> 53.59 (53.52, 53.66) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 10em; padding-left: 2em;\" indentlevel=\"1\"> μ1 </td>\n   <td style=\"text-align:center;width: 12em; \"> 19.21 (19.12, 19.31) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 10em; padding-left: 2em;\" indentlevel=\"1\"> μ2 </td>\n   <td style=\"text-align:center;width: 12em; \"> 6.32 (6.24, 6.41) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 10em; padding-left: 2em;\" indentlevel=\"1\"> μ3 </td>\n   <td style=\"text-align:center;width: 12em; \"> -0.05 (-0.05, -0.05) </td>\n  </tr>\n  <tr grouplength=\"3\"><td colspan=\"2\" style=\"border-bottom: 1px solid;\"><strong>Scale, σ</strong></td></tr>\n<tr>\n   <td style=\"text-align:left;width: 10em; padding-left: 2em;\" indentlevel=\"1\"> σ0 </td>\n   <td style=\"text-align:center;width: 12em; \"> 1.89 (1.89, 1.9) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 10em; padding-left: 2em;\" indentlevel=\"1\"> σ1 </td>\n   <td style=\"text-align:center;width: 12em; \"> 0.42 (0.41, 0.43) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 10em; padding-left: 2em;\" indentlevel=\"1\"> σ2 </td>\n   <td style=\"text-align:center;width: 12em; \"> 0.14 (0.13, 0.15) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 10em; \"> Shape, γ </td>\n   <td style=\"text-align:center;width: 12em; \"> -0.14 (-0.14, -0.13) </td>\n  </tr>\n</tbody>\n<tfoot><tr><td style=\"padding: 0; \" colspan=\"100%\">\n<span style=\"font-style: italic;\">Note: </span> <sup></sup> Negative log-likelihood (NLLH): 151553.63; Bayesian Information Criterion (BIC): 303192.96; See Equation 3 and Equation 4 for the functions used to model the location and scale parameter, respectively.</td></tr></tfoot>\n</table>\n\n`````\n:::\n:::\n\n\nThe fit statistics and model diagnostics generally suggest that the\nsecond model is a better choice between the two non-stationary\ncandidates. The model could undoubtedly be improved to better fit the\ndata, but the current version depicts broad patterns in the data\nreasonably well and allows for the exploration of model applications.\nUsing the non-stationary model, it is possible to estimate return\nperiods, return levels, and probabilities associated with extreme low\ntemperatures. For example, I could estimate the return period (or the\nprobability) for a -15° F freeze in late December. Instead, I estimated\nthe 100-year return levels for every day in March, April, October, and\nNovember on five different years contained in the data: 1900, 1940,\n1980, 2000, and 2020.\n\n\n::: {.cell layout-align=\"center\" fig.showtext='true'}\n\n```{.r .cell-code  code-fold=\"true\"}\nv1 <- make.qcov(fit2, vals = list(mu1 = cos(2*pi*60:120/365.25), mu2 = sin(2*pi*60:120/365.25), mu3 = rep(1900,61), phi1 = cos(2*pi*60:120/365.25), phi2 = sin(2*pi*60:120/365.25)))\nci100YrRLevelsMarApr1900 <- data.frame(matrix(ci(fit2, type = \"return.level\", return.period = 100, qcov = v1),ncol=4))\ncolnames(ci100YrRLevelsMarApr1900) <- c(\"ll\",\"est\",\"ul\",\"se\")\nv2 <- make.qcov(fit2, vals = list(mu1 = cos(2*pi*60:120/365.25), mu2 = sin(2*pi*60:120/365.25), mu3 = rep(1940,61), phi1 = cos(2*pi*60:120/365.25), phi2 = sin(2*pi*60:120/365.25)))\nci100YrRLevelsMarApr1940 <- data.frame(matrix(ci(fit2, type = \"return.level\", return.period = 100, qcov = v2),ncol=4))\ncolnames(ci100YrRLevelsMarApr1940) <- c(\"ll\",\"est\",\"ul\",\"se\")\nv3 <- make.qcov(fit2, vals = list(mu1 = cos(2*pi*60:120/365.25), mu2 = sin(2*pi*60:120/365.25), mu3 = rep(1980,61), phi1 = cos(2*pi*60:120/365.25), phi2 = sin(2*pi*60:120/365.25)))\nci100YrRLevelsMarApr1980 <- data.frame(matrix(ci(fit2, type = \"return.level\", return.period = 100, qcov = v3),ncol=4))\ncolnames(ci100YrRLevelsMarApr1980) <- c(\"ll\",\"est\",\"ul\",\"se\")\nv4 <- make.qcov(fit2, vals = list(mu1 = cos(2*pi*60:120/365.25), mu2 = sin(2*pi*60:120/365.25), mu3 = rep(2000,61), phi1 = cos(2*pi*60:120/365.25), phi2 = sin(2*pi*60:120/365.25)))\nci100YrRLevelsMarApr2000 <- data.frame(matrix(ci(fit2, type = \"return.level\", return.period = 100, qcov = v4),ncol=4))\ncolnames(ci100YrRLevelsMarApr2000) <- c(\"ll\",\"est\",\"ul\",\"se\")\nv5 <- make.qcov(fit2, vals = list(mu1 = cos(2*pi*60:120/365.25), mu2 = sin(2*pi*60:120/365.25), mu3 = rep(2020,61), phi1 = cos(2*pi*60:120/365.25), phi2 = sin(2*pi*60:120/365.25)))\nci100YrRLevelsMarApr2020 <- data.frame(matrix(ci(fit2, type = \"return.level\", return.period = 100, qcov = v5),ncol=4))\ncolnames(ci100YrRLevelsMarApr2020) <- c(\"ll\",\"est\",\"ul\",\"se\")\nv6 <- make.qcov(fit2, vals = list(mu1 = cos(2*pi*274:334/365.25), mu2 = sin(2*pi*274:334/365.25), mu3 = rep(1900,61), phi1 = cos(2*pi*274:334/365.25), phi2 = sin(2*pi*274:334/365.25)))\nci100YrRLevelsOctNov1900 <- data.frame(matrix(ci(fit2, type = \"return.level\", return.period = 100, qcov = v6),ncol=4))\ncolnames(ci100YrRLevelsOctNov1900) <- c(\"ll\",\"est\",\"ul\",\"se\")\nv7 <- make.qcov(fit2, vals = list(mu1 = cos(2*pi*274:334/365.25), mu2 = sin(2*pi*274:334/365.25), mu3 = rep(1940,61), phi1 = cos(2*pi*274:334/365.25), phi2 = sin(2*pi*274:334/365.25)))\nci100YrRLevelsOctNov1940 <- data.frame(matrix(ci(fit2, type = \"return.level\", return.period = 100, qcov = v7),ncol=4))\ncolnames(ci100YrRLevelsOctNov1940) <- c(\"ll\",\"est\",\"ul\",\"se\")\nv8 <- make.qcov(fit2, vals = list(mu1 = cos(2*pi*274:334/365.25), mu2 = sin(2*pi*274:334/365.25), mu3 = rep(1980,61), phi1 = cos(2*pi*274:334/365.25), phi2 = sin(2*pi*274:334/365.25)))\nci100YrRLevelsOctNov1980 <- data.frame(matrix(ci(fit2, type = \"return.level\", return.period = 100, qcov = v8),ncol=4))\ncolnames(ci100YrRLevelsOctNov1980) <- c(\"ll\",\"est\",\"ul\",\"se\")\nv9 <- make.qcov(fit2, vals = list(mu1 = cos(2*pi*274:334/365.25), mu2 = sin(2*pi*274:334/365.25), mu3 = rep(2000,61), phi1 = cos(2*pi*274:334/365.25), phi2 = sin(2*pi*274:334/365.25)))\nci100YrRLevelsOctNov2000 <- data.frame(matrix(ci(fit2, type = \"return.level\", return.period = 100, qcov = v9),ncol=4))\ncolnames(ci100YrRLevelsOctNov2000) <- c(\"ll\",\"est\",\"ul\",\"se\")\nv10 <- make.qcov(fit2, vals = list(mu1 = cos(2*pi*274:334/365.25), mu2 = sin(2*pi*274:334/365.25), mu3 = rep(2020,61), phi1 = cos(2*pi*274:334/365.25), phi2 = sin(2*pi*274:334/365.25)))\nci100YrRLevelsOctNov2020 <- data.frame(matrix(ci(fit2, type = \"return.level\", return.period = 100, qcov = v10),ncol=4))\ncolnames(ci100YrRLevelsOctNov2020) <- c(\"ll\",\"est\",\"ul\",\"se\")\n\nciRLevels <- rbind(ci100YrRLevelsMarApr1900,ci100YrRLevelsMarApr1940,ci100YrRLevelsMarApr1980,ci100YrRLevelsMarApr2000,ci100YrRLevelsMarApr2020,ci100YrRLevelsOctNov1900,ci100YrRLevelsOctNov1940,ci100YrRLevelsOctNov1980,ci100YrRLevelsOctNov2000,ci100YrRLevelsOctNov2020) |> mutate(Year = rep(factor(c(1900,1940,1980,2000,2020)),each=61,times=2), dt = rbind(data.frame(dt = rep(seq(as.Date(\"1900/03/01\"),as.Date(\"1900/04/30\"),by=\"days\"),5)),data.frame(dt = rep(seq(as.Date(\"1900/10/01\"),as.Date(\"1900/11/30\"),by=\"days\"),5))), Season = rep(factor(c(\"Spring\",\"Fall\")),each=305))\n\nciRLevels[,1:3] <- ciRLevels[,1:3] * -1\n\nggplot(data = ciRLevels, aes(x = dt$dt, y = est)) + \n  geom_line(aes(color = Year), linewidth = 1) + geom_ribbon(aes(ymin=ll,ymax=ul,fill=Year),alpha=0.2) + \n  xlab(\"Date\") + ylab(\"100-year daily low\") + facet_grid(~factor(Season, levels = c(\"Spring\",\"Fall\")), scales=\"free\") +\n  theme_nice()\n```\n\n::: {.cell-output-display}\n![Effective 100-year return levels for daily minimum temperatures in Fort Collins, CO on different dates](index_files/figure-html/fig-return-levels-1.png){#fig-return-levels fig-align='center' width=90%}\n:::\n:::\n\n\nThe 100-year freezes estimated by the model follow a predictable warming\nand cooling trend in the spring and fall, respectively, but the return\nlevels also warmed considerably, by about 5° F, over each of the\nevaluated decades. The confidence intervals are slightly larger during\ndates closer to the winter months, reflecting the reduced variability in\ndaily lows during the warmer summer months. To evaluate the model predictions, I also \n\n\n::: {.cell layout-align=\"center\" fig.showtext='true'}\n\n```{.r .cell-code  code-fold=\"true\"}\nv6 <- make.qcov(fit2, vals = list(mu1 = cos(2*pi*c(274:365,1:120)/365.25), mu2 = sin(2*pi*c(274:365,1:120)/365.25), mu3 = rep(1900,212), phi1 = cos(2*pi*c(274:365,1:120)/365.25), phi2 = sin(2*pi*c(274:365,1:120)/365.25)))\nci100YrRLevelsOctApr1900 <- data.frame(matrix(ci(fit2, type = \"return.level\", return.period = 100, qcov = v6),ncol=4))\ncolnames(ci100YrRLevelsOctApr1900) <- c(\"ll\",\"est\",\"ul\",\"se\")\nv7 <- make.qcov(fit2, vals = list(mu1 = cos(2*pi*c(274:365,1:120)/365.25), mu2 = sin(2*pi*c(274:365,1:120)/365.25), mu3 = rep(1940,212), phi1 = cos(2*pi*c(274:365,1:120)/365.25), phi2 = sin(2*pi*c(274:365,1:120)/365.25)))\nci100YrRLevelsOctApr1940 <- data.frame(matrix(ci(fit2, type = \"return.level\", return.period = 100, qcov = v7),ncol=4))\ncolnames(ci100YrRLevelsOctApr1940) <- c(\"ll\",\"est\",\"ul\",\"se\")\nv8 <- make.qcov(fit2, vals = list(mu1 = cos(2*pi*c(274:365,1:120)/365.25), mu2 = sin(2*pi*c(274:365,1:120)/365.25), mu3 = rep(1980,212), phi1 = cos(2*pi*c(274:365,1:120)/365.25), phi2 = sin(2*pi*c(274:365,1:120)/365.25)))\nci100YrRLevelsOctApr1980 <- data.frame(matrix(ci(fit2, type = \"return.level\", return.period = 100, qcov = v8),ncol=4))\ncolnames(ci100YrRLevelsOctApr1980) <- c(\"ll\",\"est\",\"ul\",\"se\")\nv9 <- make.qcov(fit2, vals = list(mu1 = cos(2*pi*c(274:365,1:120)/365.25), mu2 = sin(2*pi*c(274:365,1:120)/365.25), mu3 = rep(2000,212), phi1 = cos(2*pi*c(274:365,1:120)/365.25), phi2 = sin(2*pi*c(274:365,1:120)/365.25)))\nci100YrRLevelsOctApr2000 <- data.frame(matrix(ci(fit2, type = \"return.level\", return.period = 100, qcov = v9),ncol=4))\ncolnames(ci100YrRLevelsOctApr2000) <- c(\"ll\",\"est\",\"ul\",\"se\")\nv10 <- make.qcov(fit2, vals = list(mu1 = cos(2*pi*c(274:365,1:120)/365.25), mu2 = sin(2*pi*c(274:365,1:120)/365.25), mu3 = rep(2020,212), phi1 = cos(2*pi*c(274:365,1:120)/365.25), phi2 = sin(2*pi*c(274:365,1:120)/365.25)))\nci100YrRLevelsOctApr2020 <- data.frame(matrix(ci(fit2, type = \"return.level\", return.period = 100, qcov = v10),ncol=4))\ncolnames(ci100YrRLevelsOctApr2020) <- c(\"ll\",\"est\",\"ul\",\"se\")\n\nciRLevels <- rbind(ci100YrRLevelsOctApr1900,ci100YrRLevelsOctApr1940,ci100YrRLevelsOctApr1980,ci100YrRLevelsOctApr2000,ci100YrRLevelsOctApr2020) |> mutate(Year = rep(factor(c(1900,1940,1980,2000,2020)),each=212,times=1), dt = rbind(data.frame(dt = rep(seq(as.Date(\"2022/10/01\"),as.Date(\"2023/04/30\"),by=\"days\"),5))))\n\nciRLevels[,1:3] <- ciRLevels[,1:3] * -1\n\napi <- GET(\"https://coagmet.colostate.edu/data/daily/fcl01.json?from=2022-10-01&to=2023-04-30&fields=tMax,tMin,precip\")\nFCwx2223 <- do.call(cbind.data.frame,fromJSON(rawToChar(api$content))) |> \n  mutate(dt = as.Date(time,\"%Y-%m-%d\"), Dy = yday(dt), Mn = month(dt), Year = year(dt)) |>\n  select(dt,Dy,Mn,Year,tMin) |> rename(MnT = tMin)\n\nggplot(data = ciRLevels, aes(x = dt$dt, y = est)) + \n  geom_line(aes(color = Year), linewidth = 1) + geom_ribbon(aes(ymin=ll,ymax=ul,fill=Year),alpha=0.2) + \n  geom_line(data=FCwx2223,aes(x=dt, y=MnT), size=1) + \n  xlab(\"Date\") + ylab(\"100-year daily low\") +\n  theme_nice()\n```\n\n::: {.cell-output-display}\n![Effective 100-year return levels for daily minimum temperatures in Fort Collins, CO on different dates](index_files/figure-html/fig-validation-1.png){#fig-validation fig-align='center' width=90%}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../../../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\r\n<link href=\"../../../../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}